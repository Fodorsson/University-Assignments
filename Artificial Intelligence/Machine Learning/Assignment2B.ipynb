{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Part B\n",
    "Name: David Fodor\n",
    "Student code: B00796884\n",
    "Email: fodor-d@ulster.ac.uk\n",
    "# Reinforcement Learning in GridWorld\n",
    "\n",
    "<img src=\"gridworld.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "## Policy Evaluation\n",
    "The above diagram specifies a policy in GridWorld, i.e. directions for what action to perform in each state. In this submission, the Temporal Difference Learning approach is going to be used instead of the Monte Carlo approach used in the 'Week_11_RL_GridWorld.ipynb' file from Blackboard.\n",
    "\n",
    "This code is a modified version of code available at https://github.com/MJeremy2017/Reinforcement-Learning-Implementation\n",
    "The MIT License for the original code has been include with this version on Blackboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (0, 3)\n",
    "LOSE_STATE = (1, 3)\n",
    "START = (2, 3)\n",
    "DETERMINISTIC = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining GridWorld\n",
    "The State class below describes the GridWorld environment, including the transition probabilities (see the functions `nxtPosition` and `_chooseActionProb`) and the rewards in each state (see the function `giveReward`). Note that the environment is not deterministic. If the agent chooses direction \"up\" there is a 10% chance that actual direction will be \"left\" and 10% that it will be \"right\". The same goes for the other directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "        self.determine = DETERMINISTIC\n",
    "        \n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return -1\n",
    "        else:\n",
    "            return -0.04\n",
    "    \n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "\n",
    "    def _chooseActionProb(self, action):\n",
    "        if action == \"up\":\n",
    "            return np.random.choice([\"up\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n",
    "        if action == \"down\":\n",
    "            return np.random.choice([\"down\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n",
    "        if action == \"left\":\n",
    "            return np.random.choice([\"left\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n",
    "        if action == \"right\":\n",
    "            return np.random.choice([\"right\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n",
    "        \n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position on board\n",
    "        \"\"\"\n",
    "        if self.determine:\n",
    "            if action == \"up\":\n",
    "                nxtState = (self.state[0]-1, self.state[1])\n",
    "            elif action == \"down\":\n",
    "                nxtState = (self.state[0]+1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                nxtState = (self.state[0], self.state[1]-1)\n",
    "            else:\n",
    "                nxtState = (self.state[0], self.state[1]+1)\n",
    "            self.determine = False\n",
    "        else:\n",
    "            # non-deterministic\n",
    "            action = self._chooseActionProb(action)\n",
    "            self.determine = True\n",
    "            nxtState = self.nxtPosition(action)\n",
    "                        \n",
    "        # if next state is legal\n",
    "        if (nxtState[0] >= 0) and (nxtState[0] <= 2):\n",
    "            if (nxtState[1] >= 0) and (nxtState[1] <= 3):\n",
    "                if nxtState != (1, 1):\n",
    "                    return nxtState\n",
    "        return self.state\n",
    "    \n",
    "    def showBoard(self):\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Agent\n",
    "The Agent class below defines the policy adopted by the agent (see the function `chooseAction`) as well as the Temporal Difference Learning approach used for policy evaluation in the function `play`. A 'round' corresponds to one episode in GridWorld, i.e. from the start state to one of the end states. Many rounds need to be played to evaluate the policy. Note also that a learning rate has been defined `self.lr`. This is the $\\alpha$ value used in learning. Recall that the Temporal Difference Learning approach can be expressed as:\n",
    "\\begin{equation} \n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma * V(S_{t+1}) - V(S_t)]  \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $V(S_t)$ is the estimate of the value function $v_\\pi$ in state $S_t$, $V(S_{t+1})$ is the estimate of the value function $v_\\pi$ in state $S_{t+1}$ and $R_{t+1}$ is the reward at the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []  # record position and action taken at the position\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "        self.lr = 0.1\n",
    "        self.exp_rate = 0.3\n",
    "        self.decay_gamma = 1\n",
    "\n",
    "        # initial state values\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_values[(i, j)] = np.random.random()\n",
    "        \n",
    "        # state values are set to zero for the end states and (1,1) since the \n",
    "        # agent can never go there\n",
    "        self.state_values[WIN_STATE] = 0\n",
    "        self.state_values[LOSE_STATE] = 0\n",
    "        self.state_values[(1,1)] = 0\n",
    "        \n",
    "        # initial state counts\n",
    "        self.state_counts = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_counts[(i, j)] = 0\n",
    "        \n",
    "        self.state_counts[self.State.state] = 1\n",
    "        \n",
    "    def chooseAction(self):\n",
    "        # Choose action based on specified policy\n",
    "        if self.State.state == (2,0) or self.State.state == (1,0) or self.State.state == (1,2):\n",
    "            action = \"up\"\n",
    "        elif self.State.state == (0,0) or self.State.state == (0,1) or self.State.state == (0,2):\n",
    "            action = \"right\"\n",
    "        else:\n",
    "            action = \"left\"\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(action)\n",
    "        # update State\n",
    "        return State(state=position)     \n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.State = State()\n",
    "        self.state_counts[self.State.state] = self.state_counts[self.State.state] + 1\n",
    "        self.isEnd = self.State.isEnd\n",
    "    \n",
    "    def play(self, rounds=10):\n",
    "        i = 0\n",
    "        while i < rounds:\n",
    "            if self.State.isEnd:\n",
    "                # If the agent has reached an end state then apply the Monte Carlo approach\n",
    "                # The commented out code below is a better way to do it, but the code below\n",
    "                # that might be more helpful for the exercise.\n",
    "                '''\n",
    "                gt = self.State.giveReward()\n",
    "                for s in reversed(self.states):\n",
    "                    self.state_values[s.state] = self.state_values[s.state] + self.lr*(gt - self.state_values[s.state])\n",
    "                    gt = gt + s.giveReward()\n",
    "                '''\n",
    "                # Implementation of Monte Carlo approach\n",
    "                # We need to calculate Gt for each step. The first step requires adding all the rewards after\n",
    "                # the first action. So for each action we need the reward at the next step.\n",
    "                gt = 0\n",
    "                for j in range(len(self.states)): # self.states includes all the states in this round\n",
    "                    if j == len(self.states)-1:\n",
    "                        snext = self.State        # self.State is the final state\n",
    "                    else:\n",
    "                        snext = self.states[j+1]\n",
    "#                   gt = gt + snext.giveReward()  # add all the rewards\n",
    "                    \n",
    "                    #The five lines below implement the Temporal Difference Learning formula\n",
    "                    #We update our estimates of the value function inside the loop in this case, instead of after it\n",
    "                    #Since this formula replaces the Monte Carlo approach, the lines tied to the latter are commented out\n",
    "                    RtPlus1 = snext.giveReward()\n",
    "                    VStPlus1 = self.state_values[snext.state]\n",
    "                    s = self.states[j]\n",
    "                    #The line below is the key line for implementing the Temporal Difference Learning formula\n",
    "                    self.state_values[s.state] = self.state_values[s.state] + self.lr*(RtPlus1 + self.decay_gamma * VStPlus1 - self.state_values[s.state])\n",
    "\n",
    "#               # Now use the rewards to update our estimates of the value function\n",
    "#                for s in self.states:\n",
    "#                    self.state_values[s.state] = self.state_values[s.state] + self.lr*(gt - self.state_values[s.state])\n",
    "#                    gt = gt - s.giveReward()      # the first reward will not apply on the next step so remove it\n",
    "                \n",
    "                # This just lets us see what the final reward is for this state -1 or +1\n",
    "                reward = self.State.giveReward()\n",
    "                print(\"Game End Reward\", reward)\n",
    "                \n",
    "                self.reset()\n",
    "                i += 1\n",
    "            else:\n",
    "                action = self.chooseAction()\n",
    "                # append trace\n",
    "                self.states.append(self.State)\n",
    "                print(\"current position {} action {}\".format(self.State.state, action))\n",
    "                # by taking the action, it reaches the next state\n",
    "                self.State = self.takeAction(action)\n",
    "                # mark is end\n",
    "                self.State.isEndFunc()\n",
    "                print(\"nxt state\", self.State.state)\n",
    "                print(\"---------------------\")\n",
    "                self.isEnd = self.State.isEnd\n",
    "                # Increment count\n",
    "                self.state_counts[self.State.state] = self.state_counts[self.State.state] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent \n",
    "Display initial value function estimates (set randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.5529312683262151,\n",
       " (0, 1): 0.44607110449534815,\n",
       " (0, 2): 0.7427619374922747,\n",
       " (0, 3): 0,\n",
       " (1, 0): 0.4842800432156288,\n",
       " (1, 1): 0,\n",
       " (1, 2): 0.2398790178938207,\n",
       " (1, 3): 0,\n",
       " (2, 0): 0.44268128298990783,\n",
       " (2, 1): 0.14873532941969136,\n",
       " (2, 2): 0.20507839383578763,\n",
       " (2, 3): 0.9003486567037694}"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag = Agent()\n",
    "ag.state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Learning\n",
    "Display the sequence of states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current position (2, 3) action left\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 3) action left\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n"
     ]
    }
   ],
   "source": [
    "ag.play(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Estimated Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.6915539243090698,\n",
       " (0, 1): 0.8338118353783813,\n",
       " (0, 2): 0.926232108835523,\n",
       " (0, 3): 0,\n",
       " (1, 0): 0.5790349159992736,\n",
       " (1, 1): 0,\n",
       " (1, 2): 0.5230695567556738,\n",
       " (1, 3): 0,\n",
       " (2, 0): 0.4721785000127598,\n",
       " (2, 1): 0.3858862712455421,\n",
       " (2, 2): 0.321888758604263,\n",
       " (2, 3): 0.16886866612774665}"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "I have concluded that from 37 plays and onward, the algorithm produces a consistently correct result regarding the order of the 3 most valuable cells (0, 2), (0, 1), (0, 0), which satisfies the defined policy. Anything below 37 plays has an increasing risk of finding an erratic descending order of value estimates for the 3 most valuable cells. (The estimated values of the rest of the cells can still be wrong at 37 plays. More plays result in more accurate estmates for all cells on the grid.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative policy\n",
    "A better policy would be to change cell (2, 2) into having an arrow pointing up instead of left. This would result in a shorter route. I was unsuccessful in trying to implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning from an employability perspective\n",
    "Nowadays the advancement in AI is apparent to everyone, for it surrounds us in our daily lives. Ranging from voice assistants to the YouTube algorithm, AI has many practical uses and it is becoming ever more widespread. Of course this growing frequency of practical uses inevitably causes higher demand for AI professionals in the job market (Culbertson, 2018), especially people with machine learning skills, to be specific. Some noticeable rising job trends are related to deep learning and natural language processing (Terra, 2020), but every industry is bound to undergo machine learning reformation sooner or later.\n",
    "In contrast with the rise in demand for machine learning experts, the supply seems to be levelling off, causing competition between employers and promising top salary for future employees (Zafarino, 2018).\n",
    "For the reason that the number of machine learning candidates is finite, we can notice tech giants like Google trying to incentivise people to consider this field by making the knowledge widely available. Of course being a machine learning expert is not an easy task, it requires a lot of dedication, time, effort, and lastly, affinity. Regardless, should one choose this career option, it looks like the knowledge they have to learn for it will not be outdated anytime soon.\n",
    "\n",
    "#### References\n",
    "Daniel Culbertson. (2018). Demand for AI Talent on the Rise. [online] Available at: https://www.hiringlab.org/2018/03/01/demand-ai-talent-rise [Accessed 11 Dec. 2020].\n",
    "John Terra. (2020). The Rise of Artificial Intelligence and Machine Learning Job Trends in 2021. [online] Available at: https://www.simplilearn.com/rise-of-ai-and-machine-learning-job-trends-article [Accessed 08 Dec. 2020].\n",
    "Stephen Zafarino. (2018). The outlook for machine learning in tech: ML and AI skills in high demand. [online] Available at: https://www.cio.com/article/3293019/the-outlook-for-machine-learning-in-tech-ml-and-ai-skills-in-high-demand.html [Accessed 11 Dec. 2020].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
